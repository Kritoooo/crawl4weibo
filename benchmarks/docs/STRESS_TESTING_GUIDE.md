# crawl4weibo å‹åŠ›æµ‹è¯•æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•å¯¹ crawl4weibo è¿›è¡Œ**çœŸå®åœºæ™¯**çš„å‹åŠ›æµ‹è¯•å’Œæ€§èƒ½è¯„ä¼°ã€‚

## æ ¸å¿ƒæ€æƒ³

æœ¬æµ‹è¯•å·¥å…·æ¨¡æ‹ŸçœŸå®çš„çˆ¬è™«å·¥ä½œæµç¨‹ï¼š

```
1. æœç´¢è¯é¢˜/å…³é”®è¯ â†’ 2. æå–åšæ–‡ BID â†’ 3. çˆ¬å–è¯¦ç»†å†…å®¹ â†’ 4. ç»Ÿè®¡ååé‡
```

**ä¸»è¦æ€§èƒ½æŒ‡æ ‡ï¼šæ¯ç§’çˆ¬å–çš„åšæ–‡æ•°é‡ (posts/second)**

## ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æµ‹è¯•æ¨¡å¼](#æµ‹è¯•æ¨¡å¼)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [æ€§èƒ½æŒ‡æ ‡è¯´æ˜](#æ€§èƒ½æŒ‡æ ‡è¯´æ˜)
- [å®æˆ˜ç¤ºä¾‹](#å®æˆ˜ç¤ºä¾‹)
- [ç»“æœåˆ†æ](#ç»“æœåˆ†æ)

## å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: å‘½ä»¤è¡Œäº¤äº’å¼æµ‹è¯•ï¼ˆæ¨èï¼‰

```bash
cd benchmarks
python test_performance.py
```

æŒ‰æç¤ºè¾“å…¥ï¼š
- Cookiesï¼ˆå¯é€‰ï¼‰
- ä»£ç† API URLï¼ˆå¯é€‰ï¼‰
- æœç´¢å…³é”®è¯ï¼ˆå¦‚ï¼š"Python"ï¼‰
- æµ‹è¯•ç±»å‹ï¼ˆ1-4ï¼‰

### æ–¹æ³• 2: ä½¿ç”¨ pytest

```bash
# è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
pytest benchmarks/test_performance.py -v -m integration

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest benchmarks/test_performance.py::test_sequential_topic_crawl -v
pytest benchmarks/test_performance.py::test_concurrent_topic_crawl -v
pytest benchmarks/test_performance.py::test_timed_crawl -v
pytest benchmarks/test_performance.py::test_proxy_comparison -v
```

### æ–¹æ³• 3: ç¼–ç¨‹æ–¹å¼

```python
from benchmarks.test_performance import TopicCrawler

# åˆå§‹åŒ–çˆ¬è™«
crawler = TopicCrawler(
    cookies="your_cookies_here",
    proxy_api_url="http://proxy-api.com/get"
)

# è¿è¡Œé¡ºåºæµ‹è¯•
metrics = crawler.crawl_topic_sequential(
    query="Pythonç¼–ç¨‹",
    search_pages=2,
    max_posts=20
)

# æ‰“å°ç»“æœ
metrics.print_summary()

# æŸ¥çœ‹å…³é”®æŒ‡æ ‡
print(f"ååé‡: {metrics.posts_per_second:.3f} posts/s")
print(f"æˆåŠŸç‡: {metrics.overall_success_rate:.2f}%")
```

## æµ‹è¯•æ¨¡å¼

### 1. é¡ºåºçˆ¬å– (Sequential Crawl)

å•çº¿ç¨‹é¡ºåºçˆ¬å–ï¼Œç”¨äºæµ‹è¯•åŸºç¡€æ€§èƒ½ã€‚

```python
metrics = crawler.crawl_topic_sequential(
    query="äººå·¥æ™ºèƒ½",      # æœç´¢å…³é”®è¯
    search_pages=2,       # æœç´¢2é¡µ
    max_posts=20          # æœ€å¤šçˆ¬å–20æ¡åšæ–‡
)
```

**å·¥ä½œæµç¨‹ï¼š**
1. æœç´¢ç¬¬1é¡µ â†’ æå– BIDs â†’ æœç´¢ç¬¬2é¡µ â†’ æå– BIDs
2. é€ä¸ªçˆ¬å–æ¯æ¡åšæ–‡çš„è¯¦ç»†å†…å®¹

**é€‚ç”¨åœºæ™¯ï¼š**
- äº†è§£å•å®¢æˆ·ç«¯çš„åŸºå‡†æ€§èƒ½
- è°ƒè¯•å’Œé—®é¢˜æ’æŸ¥
- éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§

### 2. å¹¶å‘çˆ¬å– (Concurrent Crawl)

å¤šçº¿ç¨‹å¹¶å‘çˆ¬å–è¯¦æƒ…ï¼Œæå‡ååé‡ã€‚

```python
metrics = crawler.crawl_topic_concurrent(
    query="Python",
    search_pages=3,
    max_posts=50,
    num_workers=10        # 10ä¸ªå·¥ä½œçº¿ç¨‹å¹¶å‘çˆ¬å–
)
```

**å·¥ä½œæµç¨‹ï¼š**
1. é¡ºåºæœç´¢ï¼Œæ”¶é›†æ‰€æœ‰ BIDs
2. ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘çˆ¬å–è¯¦æƒ…

**é€‚ç”¨åœºæ™¯ï¼š**
- æµ‹è¯•æœ€å¤§ååé‡
- è¯„ä¼°å¹¶å‘æ€§èƒ½
- æ¨¡æ‹Ÿå¤šç”¨æˆ·åœºæ™¯

### 3. å®šæ—¶çˆ¬å– (Timed Crawl)

åœ¨å›ºå®šæ—¶é—´å†…æŒç»­çˆ¬å–ï¼Œæµ‹è¯•ç¨³å®šæ€§ã€‚

```python
metrics = crawler.crawl_topic_timed(
    query="ç§‘æŠ€",
    duration_seconds=60,  # æŒç»­60ç§’
    num_workers=5,        # 5ä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹
    search_pages=5        # é¢„å…ˆæœç´¢5é¡µå»ºç«‹BIDæ± 
)
```

**å·¥ä½œæµç¨‹ï¼š**
1. é¢„å…ˆæœç´¢å¤šé¡µï¼Œå»ºç«‹ BID æ± 
2. åœ¨æŒ‡å®šæ—¶é—´å†…å¾ªç¯çˆ¬å– BID æ± ä¸­çš„åšæ–‡
3. å®æ—¶æ˜¾ç¤ºçˆ¬å–é€Ÿç‡

**é€‚ç”¨åœºæ™¯ï¼š**
- æµ‹è¯•æŒç»­è¿è¡Œçš„ç¨³å®šæ€§
- è¯„ä¼°é•¿æ—¶é—´çš„å¹³å‡ååé‡
- å‘ç°å†…å­˜æ³„æ¼ç­‰é—®é¢˜

### 4. ä»£ç†å¯¹æ¯” (Proxy Comparison)

å¯¹æ¯”ä½¿ç”¨ä»£ç†å’Œä¸ä½¿ç”¨ä»£ç†çš„æ€§èƒ½å·®å¼‚ã€‚

```python
results = crawler.compare_proxy_performance(
    query="å¾®åš",
    search_pages=1,
    max_posts=10
)

# ç»“æœåŒ…å«ä¸¤ä¸ªæŒ‡æ ‡å¯¹è±¡
# results['no_proxy']    - æ— ä»£ç†
# results['with_proxy']  - ä½¿ç”¨ä»£ç†
```

**é€‚ç”¨åœºæ™¯ï¼š**
- è¯„ä¼°ä»£ç†å¯¹æ€§èƒ½çš„å½±å“
- é€‰æ‹©æœ€ä¼˜ä»£ç†æœåŠ¡å•†
- æˆæœ¬æ”¶ç›Šåˆ†æ

## æ€§èƒ½æŒ‡æ ‡è¯´æ˜

### ä¸»è¦æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | è®¡ç®—æ–¹å¼ |
|------|------|----------|
| **Posts Crawled/Second** | **æ¯ç§’çˆ¬å–åšæ–‡æ•°**ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰ | çˆ¬å–æˆåŠŸçš„åšæ–‡æ•° / æ€»è€—æ—¶ |
| Total Posts Crawled | æˆåŠŸçˆ¬å–çš„åšæ–‡æ€»æ•° | - |
| Total Time | æ€»è€—æ—¶ï¼ˆç§’ï¼‰ | - |

### æœç´¢é˜¶æ®µæŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| Search Requests | æœç´¢è¯·æ±‚æ¬¡æ•° |
| Search Success Rate | æœç´¢æˆåŠŸç‡ (%) |
| Posts Found (BIDs) | æ‰¾åˆ°çš„åšæ–‡ BID æ€»æ•° |
| Avg Search Time | å¹³å‡æœç´¢å“åº”æ—¶é—´ (ç§’) |

### è¯¦æƒ…çˆ¬å–é˜¶æ®µæŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| Detail Requests | è¯¦æƒ…è¯·æ±‚æ¬¡æ•° |
| Detail Success Rate | è¯¦æƒ…çˆ¬å–æˆåŠŸç‡ (%) |
| Avg Detail Time | å¹³å‡è¯¦æƒ…çˆ¬å–æ—¶é—´ (ç§’) |
| P50/P95/P99 | å“åº”æ—¶é—´ç™¾åˆ†ä½æ•° |

### ç¤ºä¾‹è¾“å‡º

```
================================================================================
                        Crawling Performance Test
================================================================================

ğŸ¯ PRIMARY METRIC
Posts Crawled/Second:     0.156 posts/s
Total Posts Crawled:      18
Total Time:               115.23s

ğŸ“Š SEARCH PHASE
Search Requests:          2
  Successful:             2
  Failed:                 0
  Success Rate:           100.00%
Posts Found (BIDs):       20
Avg Search Time:          3.45s
Total Search Time:        6.90s

ğŸ“ DETAIL CRAWLING PHASE
Detail Requests:          20
  Successful:             18
  Failed:                 2
  Success Rate:           90.00%
Avg Detail Time:          5.42s
Total Detail Time:        108.33s

ğŸ“ˆ OVERALL STATISTICS
Total Requests:           22
Overall Success Rate:     90.91%
Unique Posts Crawled:     18
Failed Posts:             2

â±ï¸  DETAIL RESPONSE TIME PERCENTILES
  Min:                    2.34s
  P50 (Median):           5.12s
  P95:                    9.87s
  P99:                    11.23s
  Max:                    11.45s

âŒ ERROR SUMMARY
  ParseError: 1
  NetworkError: 1
================================================================================
```

## å®æˆ˜ç¤ºä¾‹

### åœºæ™¯ 1: è¯„ä¼°åŸºç¡€çˆ¬å–èƒ½åŠ›

æµ‹è¯•çˆ¬å–"Pythonç¼–ç¨‹"è¯é¢˜çš„åšæ–‡ï¼Œäº†è§£åŸºå‡†æ€§èƒ½ã€‚

```python
from benchmarks.test_performance import TopicCrawler

crawler = TopicCrawler(cookies="your_cookies")

# é¡ºåºçˆ¬å–
metrics = crawler.crawl_topic_sequential(
    query="Pythonç¼–ç¨‹",
    search_pages=2,      # æœç´¢2é¡µ
    max_posts=20         # æœ€å¤š20æ¡
)

metrics.print_summary()

# é¢„æœŸç»“æœï¼š
# - ååé‡çº¦ 0.15-0.25 posts/sï¼ˆè€ƒè™‘å†…ç½®å»¶è¿Ÿï¼‰
# - æˆåŠŸç‡ > 90%
# - å¹³å‡è¯¦æƒ…æ—¶é—´ 3-6ç§’
```

### åœºæ™¯ 2: æµ‹è¯•å¹¶å‘æ€§èƒ½æå‡

å¯¹æ¯”ä¸åŒå¹¶å‘æ•°çš„ååé‡æå‡ã€‚

```python
query = "äººå·¥æ™ºèƒ½"
search_pages = 3
max_posts = 50

# å•çº¿ç¨‹
metrics_1 = crawler.crawl_topic_sequential(query, search_pages, max_posts)

# 5çº¿ç¨‹å¹¶å‘
metrics_5 = crawler.crawl_topic_concurrent(query, search_pages, max_posts, num_workers=5)

# 10çº¿ç¨‹å¹¶å‘
metrics_10 = crawler.crawl_topic_concurrent(query, search_pages, max_posts, num_workers=10)

# å¯¹æ¯”ç»“æœ
print(f"\nå¹¶å‘æ€§èƒ½å¯¹æ¯”:")
print(f"å•çº¿ç¨‹:   {metrics_1.posts_per_second:.3f} posts/s")
print(f"5å¹¶å‘:    {metrics_5.posts_per_second:.3f} posts/s (æå‡ {metrics_5.posts_per_second/metrics_1.posts_per_second:.1f}x)")
print(f"10å¹¶å‘:   {metrics_10.posts_per_second:.3f} posts/s (æå‡ {metrics_10.posts_per_second/metrics_1.posts_per_second:.1f}x)")
```

### åœºæ™¯ 3: æŒç»­å‹åŠ›æµ‹è¯•

è¿è¡Œ5åˆ†é’Ÿçš„æŒç»­çˆ¬å–ï¼Œæµ‹è¯•ç¨³å®šæ€§ã€‚

```python
# 5åˆ†é’ŸæŒç»­å‹åŠ›æµ‹è¯•
metrics = crawler.crawl_topic_timed(
    query="ç§‘æŠ€æ–°é—»",
    duration_seconds=300,    # 5åˆ†é’Ÿ
    num_workers=5,
    search_pages=10          # é¢„å…ˆæœç´¢10é¡µå»ºç«‹å¤§çš„BIDæ± 
)

metrics.print_summary("5åˆ†é’ŸæŒç»­å‹åŠ›æµ‹è¯•")

# å…³æ³¨æŒ‡æ ‡ï¼š
# - ååé‡æ˜¯å¦ç¨³å®š
# - æˆåŠŸç‡æ˜¯å¦ä¸‹é™
# - æ˜¯å¦æœ‰å†…å­˜æ³„æ¼
# - é”™è¯¯ç‡æ˜¯å¦ä¸Šå‡
```

### åœºæ™¯ 4: ä»£ç†æ± æ€§èƒ½è¯„ä¼°

è¯„ä¼°ä»£ç†å¯¹æ€§èƒ½çš„å½±å“ã€‚

```python
# å¯¹æ¯”ä¸åŒä»£ç†é…ç½®
proxy_services = [
    ("http://proxy1.com/api/get", "æœåŠ¡å•†A"),
    ("http://proxy2.com/api/get", "æœåŠ¡å•†B"),
]

for proxy_url, name in proxy_services:
    crawler = TopicCrawler(cookies="your_cookies", proxy_api_url=proxy_url)

    results = crawler.compare_proxy_performance(
        query="Python",
        search_pages=2,
        max_posts=20
    )

    print(f"\n{name} æ€§èƒ½:")
    print(f"  æ— ä»£ç†: {results['no_proxy'].posts_per_second:.3f} posts/s, "
          f"{results['no_proxy'].overall_success_rate:.2f}% æˆåŠŸç‡")
    print(f"  æœ‰ä»£ç†: {results['with_proxy'].posts_per_second:.3f} posts/s, "
          f"{results['with_proxy'].overall_success_rate:.2f}% æˆåŠŸç‡")
```

### åœºæ™¯ 5: å¯»æ‰¾æœ€ä¼˜å¹¶å‘æ•°

æ‰¾å‡ºæ€§èƒ½æœ€ä½³çš„å¹¶å‘çº¿ç¨‹æ•°ã€‚

```python
query = "å¾®åšçƒ­æœ"
search_pages = 5
max_posts = 100

results = {}
worker_counts = [1, 3, 5, 10, 15, 20]

for workers in worker_counts:
    print(f"\næµ‹è¯• {workers} ä¸ªå·¥ä½œçº¿ç¨‹...")
    metrics = crawler.crawl_topic_concurrent(
        query=query,
        search_pages=search_pages,
        max_posts=max_posts,
        num_workers=workers
    )
    results[workers] = metrics

# æ‰¾å‡ºæœ€ä¼˜é…ç½®
best_workers = max(results.items(), key=lambda x: x[1].posts_per_second)
print(f"\næœ€ä¼˜é…ç½®: {best_workers[0]} ä¸ªå·¥ä½œçº¿ç¨‹")
print(f"ååé‡: {best_workers[1].posts_per_second:.3f} posts/s")
print(f"æˆåŠŸç‡: {best_workers[1].overall_success_rate:.2f}%")

# ç»˜åˆ¶æ€§èƒ½æ›²çº¿
print(f"\n{'Workers':<10} {'Posts/s':<15} {'Success Rate':<15}")
print("-" * 40)
for workers, metrics in sorted(results.items()):
    print(f"{workers:<10} {metrics.posts_per_second:<15.3f} {metrics.overall_success_rate:<15.2f}%")
```

## ç»“æœåˆ†æ

### å¥åº·æŒ‡æ ‡ âœ…

- **ååé‡**:
  - å•çº¿ç¨‹ > 0.15 posts/s
  - 5å¹¶å‘ > 0.5 posts/s
  - 10å¹¶å‘ > 0.8 posts/s

- **æˆåŠŸç‡**:
  - æœç´¢æˆåŠŸç‡ > 95%
  - è¯¦æƒ…æˆåŠŸç‡ > 90%
  - æ€»ä½“æˆåŠŸç‡ > 90%

- **å“åº”æ—¶é—´**:
  - P95 < 10ç§’
  - P99 < 15ç§’

### è­¦å‘ŠæŒ‡æ ‡ âš ï¸

- æˆåŠŸç‡ 80-90%
- ååé‡ä¸‹é™ > 30%
- P95 å“åº”æ—¶é—´ 10-20ç§’
- å¶å‘ ParseError æˆ– NetworkError

### é—®é¢˜æŒ‡æ ‡ âŒ

- **æˆåŠŸç‡ < 80%** â†’ å¯èƒ½è§¦å‘åçˆ¬è™«ï¼Œéœ€è¦é™ä½é¢‘ç‡æˆ–æ›´æ¢ä»£ç†
- **ååé‡ < 0.1 posts/s** â†’ ç½‘ç»œæˆ–ä»£ç†é—®é¢˜
- **å¤§é‡ 432 é”™è¯¯** â†’ è§¦å‘å¾®åšåçˆ¬è™«ï¼Œéœ€ç«‹å³åœæ­¢
- **P95 > 20ç§’** â†’ ä»£ç†è´¨é‡å·®æˆ–ç½‘ç»œé—®é¢˜

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### æå‡ååé‡
1. **å¢åŠ å¹¶å‘æ•°**ï¼šæµ‹è¯•æ‰¾å‡ºæœ€ä¼˜å¹¶å‘æ•°ï¼ˆé€šå¸¸ 5-10 ä¸ªçº¿ç¨‹ï¼‰
2. **ä½¿ç”¨é«˜è´¨é‡ä»£ç†**ï¼šé€‰æ‹©å“åº”å¿«ã€æˆåŠŸç‡é«˜çš„ä»£ç†
3. **é¢„å…ˆæœç´¢æ›´å¤šé¡µ**ï¼šåœ¨å®šæ—¶æµ‹è¯•ä¸­å»ºç«‹å¤§çš„ BID æ± 

#### æé«˜æˆåŠŸç‡
1. **é™ä½è¯·æ±‚é¢‘ç‡**ï¼šå‡å°‘å¹¶å‘æ•°
2. **ä½¿ç”¨ä»£ç†æ± **ï¼šè½®æ¢ IP é¿å…å°ç¦
3. **æ·»åŠ é‡è¯•é€»è¾‘**ï¼šå¯¹å¤±è´¥çš„è¯·æ±‚é‡è¯•

#### é™ä½å“åº”æ—¶é—´
1. **é€‰æ‹©åœ°ç†ä½ç½®è¿‘çš„ä»£ç†**
2. **è¿‡æ»¤æ…¢é€Ÿä»£ç†**ï¼šç›‘æ§ä»£ç†å“åº”æ—¶é—´
3. **ä¼˜åŒ–ç½‘ç»œé…ç½®**ï¼šä½¿ç”¨æ›´å¿«çš„ç½‘ç»œ

## æœ€ä½³å®è·µ

### 1. æµ‹è¯•å‰å‡†å¤‡

- âœ… ç¡®ä¿ cookies æœ‰æ•ˆä¸”æœªè¿‡æœŸ
- âœ… ç½‘ç»œè¿æ¥ç¨³å®š
- âœ… å‡†å¤‡çœŸå®å­˜åœ¨çš„æœç´¢å…³é”®è¯
- âœ… äº†è§£ç›®æ ‡ API çš„é€Ÿç‡é™åˆ¶

### 2. æµ‹è¯•ç­–ç•¥

**æ¸è¿›å¼æµ‹è¯•**ï¼š
```
Step 1: é¡ºåºæµ‹è¯• (10æ¡) â†’ éªŒè¯åŠŸèƒ½
Step 2: é¡ºåºæµ‹è¯• (50æ¡) â†’ æµ‹è¯•åŸºå‡†æ€§èƒ½
Step 3: å¹¶å‘æµ‹è¯• (50æ¡, 5å¹¶å‘) â†’ æµ‹è¯•å¹¶å‘æ€§èƒ½
Step 4: å®šæ—¶æµ‹è¯• (60ç§’) â†’ æµ‹è¯•ç¨³å®šæ€§
Step 5: å¹¶å‘æµ‹è¯• (100æ¡, æœ€ä¼˜å¹¶å‘æ•°) â†’ æµ‹è¯•æé™æ€§èƒ½
```

### 3. å®‰å…¨å»ºè®®

âš ï¸ **é¿å…è¿‡åº¦è¯·æ±‚**
- ä¸è¦ä½¿ç”¨è¶…è¿‡ 20 ä¸ªå¹¶å‘çº¿ç¨‹
- é•¿æ—¶é—´æµ‹è¯•å»ºè®®é—´éš”ä¼‘æ¯
- ç›‘æ§é”™è¯¯ç‡ï¼ŒåŠæ—¶åœæ­¢

âš ï¸ **ä¿æŠ¤è´¦å·å®‰å…¨**
- ä½¿ç”¨æµ‹è¯•è´¦å·ï¼Œä¸ç”¨ä¸»è´¦å·
- ä¸è¦åœ¨å…¬å…±ç¯å¢ƒæš´éœ² cookies
- å®šæœŸæ›´æ¢ cookies

âš ï¸ **éµå®ˆè§„åˆ™**
- å°Šé‡ robots.txt
- ä¸å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
- éµå®ˆç½‘ç«™æœåŠ¡æ¡æ¬¾

### 4. ç»“æœè®°å½•

å»ºè®®è®°å½•æ¯æ¬¡æµ‹è¯•çš„å…³é”®ä¿¡æ¯ï¼š

```python
# æµ‹è¯•è®°å½•æ¨¡æ¿
test_record = {
    "date": "2025-01-15",
    "query": "Pythonç¼–ç¨‹",
    "mode": "concurrent",
    "workers": 5,
    "max_posts": 50,
    "proxy": "proxy-service-A",
    "results": {
        "posts_per_second": 0.625,
        "success_rate": 92.5,
        "avg_detail_time": 4.8,
        "total_time": 80.0
    },
    "notes": "ä½¿ç”¨æ–°ä»£ç†æœåŠ¡å•†ï¼Œæ€§èƒ½æå‡æ˜æ˜¾"
}
```

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆååé‡å¾ˆä½ï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
- å†…ç½®å»¶è¿Ÿï¼ˆ1-3ç§’ï¼‰é™åˆ¶äº†é€Ÿåº¦
- ç½‘ç»œå»¶è¿Ÿé«˜
- ä»£ç†é€Ÿåº¦æ…¢
- å¹¶å‘æ•°ä¸è¶³

**è§£å†³æ–¹æ¡ˆï¼š**
- å¢åŠ å¹¶å‘æ•°ï¼ˆæ¨è 5-10ï¼‰
- ä½¿ç”¨æ›´å¿«çš„ä»£ç†
- æ£€æŸ¥ç½‘ç»œçŠ¶å†µ

### Q2: ä¸ºä»€ä¹ˆæˆåŠŸç‡ä¸‹é™ï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
- è§¦å‘åçˆ¬è™«æœºåˆ¶
- ä»£ç†è¢«å°ç¦
- Cookies è¿‡æœŸ
- è¯·æ±‚é¢‘ç‡è¿‡é«˜

**è§£å†³æ–¹æ¡ˆï¼š**
- é™ä½å¹¶å‘æ•°å’Œè¯·æ±‚é¢‘ç‡
- æ›´æ¢ä»£ç†æˆ–ä»£ç†æœåŠ¡å•†
- æ›´æ–° cookies
- æ·»åŠ æ›´é•¿çš„å»¶è¿Ÿ

### Q3: å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¹¶å‘æ•°ï¼Ÿ

**å»ºè®®ï¼š**
- ä»å°åˆ°å¤§é€æ­¥æµ‹è¯•ï¼š1 â†’ 3 â†’ 5 â†’ 10 â†’ 15
- è§‚å¯Ÿååé‡å’ŒæˆåŠŸç‡çš„å˜åŒ–
- é€‰æ‹©ååé‡æœ€é«˜ä¸”æˆåŠŸç‡ > 90% çš„é…ç½®
- é€šå¸¸ 5-10 ä¸ªå¹¶å‘æ˜¯æœ€ä¼˜çš„

### Q4: å®šæ—¶æµ‹è¯•ä¸ºä»€ä¹ˆè¦é¢„å…ˆæœç´¢ï¼Ÿ

**åŸå› ï¼š**
- æœç´¢æ“ä½œç›¸å¯¹è€—æ—¶ï¼ˆæ¯é¡µ 3-5 ç§’ï¼‰
- é¢„å…ˆæœç´¢å¯ä»¥å»ºç«‹ BID æ± 
- æµ‹è¯•é˜¶æ®µä¸“æ³¨äºè¯¦æƒ…çˆ¬å–æ€§èƒ½
- é¿å…é‡å¤æœç´¢ç›¸åŒå†…å®¹

### Q5: å¦‚ä½•è§£è¯»å“åº”æ—¶é—´ç™¾åˆ†ä½æ•°ï¼Ÿ

**å«ä¹‰ï¼š**
- P50ï¼ˆä¸­ä½æ•°ï¼‰ï¼š50% çš„è¯·æ±‚åœ¨æ­¤æ—¶é—´å†…å®Œæˆ
- P95ï¼š95% çš„è¯·æ±‚åœ¨æ­¤æ—¶é—´å†…å®Œæˆ
- P99ï¼š99% çš„è¯·æ±‚åœ¨æ­¤æ—¶é—´å†…å®Œæˆ

**å»ºè®®ï¼š**
- å…³æ³¨ P95 å’Œ P99ï¼Œå®ƒä»¬åæ˜ æç«¯æƒ…å†µ
- P95 > 10ç§’ éœ€è¦ä¼˜åŒ–
- P99 > 20ç§’ è¯´æ˜æœ‰ä¸¥é‡é—®é¢˜

## è¿›é˜¶åŠŸèƒ½

### è‡ªå®šä¹‰æµ‹è¯•

ä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æµ‹è¯•è„šæœ¬ï¼š

```python
# ä¿®æ”¹ benchmarks/test_performance.py

class TopicCrawler:
    # æ·»åŠ è‡ªå®šä¹‰æµ‹è¯•æ–¹æ³•
    def crawl_topic_custom(self, query, custom_param):
        # ä½ çš„è‡ªå®šä¹‰é€»è¾‘
        pass
```

### é›†æˆåˆ° CI/CD

```yaml
# .github/workflows/performance-test.yml
name: Performance Tests

on:
  schedule:
    - cron: '0 2 * * 0'  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹è¿è¡Œ

jobs:
  performance:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
      - name: Run performance tests
        run: |
          pytest benchmarks/test_performance.py::test_timed_crawl -v
        env:
          WEIBO_COOKIES: ${{ secrets.WEIBO_COOKIES }}
```

### ç»“æœå¯è§†åŒ–

```python
import matplotlib.pyplot as plt

# æ”¶é›†å¤šæ¬¡æµ‹è¯•æ•°æ®
worker_counts = [1, 3, 5, 10, 15, 20]
throughputs = []

for workers in worker_counts:
    metrics = crawler.crawl_topic_concurrent(
        query="Python", search_pages=3, max_posts=50, num_workers=workers
    )
    throughputs.append(metrics.posts_per_second)

# ç»˜å›¾
plt.plot(worker_counts, throughputs, marker='o')
plt.xlabel('Number of Workers')
plt.ylabel('Posts per Second')
plt.title('Throughput vs Concurrency')
plt.grid(True)
plt.savefig('performance_curve.png')
```

## ç›¸å…³èµ„æº

- [é¡¹ç›®ä¸» README](../README.md)
- [æ¶æ„åˆ†ææ–‡æ¡£](ARCHITECTURE_ANALYSIS.md)
- [API å¿«é€Ÿå‚è€ƒ](QUICK_REFERENCE.md)

## åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ– Pull Requestï¼
