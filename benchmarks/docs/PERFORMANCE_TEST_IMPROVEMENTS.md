# æ€§èƒ½æµ‹è¯•æ”¹è¿›è¯´æ˜

## ğŸ”§ å·²ä¿®å¤çš„é—®é¢˜

### 1. âœ… Client å¤ç”¨ä¼˜åŒ–

**é—®é¢˜ï¼š** ä¹‹å‰æ¯æ¬¡çˆ¬å–ä¸€ä¸ªåšæ–‡éƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ WeiboClient å®ä¾‹ï¼Œå¯¼è‡´ï¼š
- é¢‘ç¹åˆ›å»º/é”€æ¯è¿æ¥ï¼Œæµªè´¹èµ„æº
- ä»£ç†æ± æ— æ³•æœ‰æ•ˆå¤ç”¨
- æ€§èƒ½ä¸‹é™

**è§£å†³æ–¹æ¡ˆï¼š** ç°åœ¨æ¯ä¸ª worker çº¿ç¨‹ç»´æŠ¤ä¸€ä¸ª client å®ä¾‹ï¼Œåœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…å¤ç”¨ï¼š

```python
def worker_task(worker_id: int):
    """æ¯ä¸ª worker ç»´æŠ¤ä¸€ä¸ª client"""
    worker_client = self._create_client()  # åªåˆ›å»ºä¸€æ¬¡

    while not bid_queue.empty():
        bid = bid_queue.get_nowait()
        # å¤ç”¨åŒä¸€ä¸ª client
        self._crawl_post_detail(worker_client, bid, metrics)
```

**æ”¹è¿›æ•ˆæœï¼š**
- å‡å°‘è¿æ¥å¼€é”€
- æå‡å¹¶å‘æ€§èƒ½
- ä»£ç†æ± æ›´æœ‰æ•ˆ

### 2. âœ… è¯¦ç»†é”™è¯¯ä¿¡æ¯

**é—®é¢˜ï¼š** ä¹‹å‰åªæ˜¾ç¤ºé”™è¯¯ç±»å‹ï¼Œæ— æ³•çœ‹åˆ°å…·ä½“é”™è¯¯åŸå› ï¼Œéš¾ä»¥è°ƒè¯•ã€‚

**è§£å†³æ–¹æ¡ˆï¼š** æ–°å¢ `ErrorDetail` ç±»ï¼Œè®°å½•å®Œæ•´é”™è¯¯ä¿¡æ¯ï¼š

```python
@dataclass
class ErrorDetail:
    """é”™è¯¯è¯¦æƒ…è®°å½•"""
    bid: str = ""              # å¤±è´¥çš„åšæ–‡ BID
    error_type: str = ""       # é”™è¯¯ç±»å‹
    error_message: str = ""    # å®Œæ•´é”™è¯¯æ¶ˆæ¯
    timestamp: float = 0.0     # é”™è¯¯æ—¶é—´æˆ³
```

**æ–°çš„é”™è¯¯æ˜¾ç¤ºï¼š**

```
âŒ ERROR SUMMARY

  ParseError: 5 occurrences
    Example 1: BID: Nzw8xB3k2
              Message: Post data not found in response
    Example 2: BID: Nzw7yA2j1
              Message: Invalid JSON response from server
    Example 3: BID: Nzw6pC1h9
              Message: Missing required field 'created_at'
    ... and 2 more

  NetworkError: 3 occurrences
    Example 1: BID: Nzw5mD0g8
              Message: Connection timeout after 5 seconds
    ... and 2 more

  ğŸ’¡ Tip: Call metrics.export_error_log('filename.txt') to export full error details
```

### 3. âœ… é”™è¯¯æ—¥å¿—å¯¼å‡º

æ–°å¢ `export_error_log()` æ–¹æ³•ï¼Œå¯ä»¥å¯¼å‡ºå®Œæ•´çš„é”™è¯¯æ—¥å¿—åˆ°æ–‡ä»¶ï¼š

```python
# è¿è¡Œæµ‹è¯•
metrics = crawler.crawl_topic_concurrent(
    query="Python",
    search_pages=2,
    max_posts=50,
    num_workers=5
)

# æŸ¥çœ‹æ‘˜è¦
metrics.print_summary()

# å¦‚æœæœ‰é”™è¯¯ï¼Œå¯¼å‡ºè¯¦ç»†æ—¥å¿—
if metrics.detail_failed > 0:
    metrics.export_error_log("debug_errors.txt")
```

**å¯¼å‡ºçš„æ—¥å¿—æ ¼å¼ï¼š**

```
================================================================================
CRAWL4WEIBO PERFORMANCE TEST ERROR LOG
================================================================================

DETAIL CRAWLING ERRORS:
--------------------------------------------------------------------------------

Error #1:
  BID: Nzw8xB3k2
  Type: ParseError
  Time: 1737500000.123
  Message: Failed to parse post data: Missing required field 'text'

Error #2:
  BID: Nzw7yA2j1
  Type: NetworkError
  Time: 1737500005.456
  Message: HTTPError: 432 Client Error: Too Many Requests

...

================================================================================
Total Errors: 15
================================================================================
```

## ğŸ“Š æ”¹è¿›åçš„ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨ï¼ˆè‡ªåŠ¨æ˜¾ç¤ºé”™è¯¯æ‘˜è¦ï¼‰

```python
from tests.test_performance import TopicCrawler

crawler = TopicCrawler(cookies="your_cookies")

metrics = crawler.crawl_topic_concurrent(
    query="Pythonç¼–ç¨‹",
    search_pages=3,
    max_posts=50,
    num_workers=5
)

# æ‰“å°ç»“æœï¼ˆä¼šè‡ªåŠ¨æ˜¾ç¤ºé”™è¯¯æ‘˜è¦ï¼‰
metrics.print_summary()
```

### è°ƒè¯•å¤±è´¥é—®é¢˜

```python
# è¿è¡Œæµ‹è¯•
metrics = crawler.crawl_topic_sequential(
    query="äººå·¥æ™ºèƒ½",
    search_pages=2,
    max_posts=20
)

# æŸ¥çœ‹ç»“æœ
metrics.print_summary()

# æ£€æŸ¥æˆåŠŸç‡
if metrics.detail_success_rate < 80:
    print(f"\nâš ï¸  æˆåŠŸç‡è¿‡ä½ï¼æ­£åœ¨å¯¼å‡ºé”™è¯¯æ—¥å¿—...")
    metrics.export_error_log("low_success_rate_errors.txt")

    # åˆ†æé”™è¯¯ç±»å‹
    error_types = {}
    for err in metrics.detail_errors:
        error_types[err.error_type] = error_types.get(err.error_type, 0) + 1

    print(f"\né”™è¯¯ç±»å‹åˆ†å¸ƒï¼š")
    for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
        print(f"  {error_type}: {count}")

    # ç»™å‡ºå»ºè®®
    if error_types.get('NetworkError', 0) > 5:
        print("\nğŸ’¡ å»ºè®®ï¼šNetworkError è¾ƒå¤šï¼Œå¯èƒ½æ˜¯ç½‘ç»œæˆ–ä»£ç†é—®é¢˜")
    if error_types.get('ParseError', 0) > 5:
        print("\nğŸ’¡ å»ºè®®ï¼šParseError è¾ƒå¤šï¼Œå¯èƒ½æ˜¯ API è¿”å›æ ¼å¼å˜åŒ–")
```

### å¯¹æ¯”ä¸åŒé…ç½®çš„é”™è¯¯ç‡

```python
# æµ‹è¯•ä¸åŒå¹¶å‘æ•°çš„é”™è¯¯ç‡
configs = [
    {"workers": 1, "name": "å•çº¿ç¨‹"},
    {"workers": 5, "name": "5å¹¶å‘"},
    {"workers": 10, "name": "10å¹¶å‘"},
    {"workers": 20, "name": "20å¹¶å‘"},
]

results = []
for config in configs:
    print(f"\næµ‹è¯• {config['name']}...")

    metrics = crawler.crawl_topic_concurrent(
        query="ç§‘æŠ€",
        search_pages=2,
        max_posts=30,
        num_workers=config['workers']
    )

    results.append({
        "name": config['name'],
        "success_rate": metrics.detail_success_rate,
        "error_count": metrics.detail_failed,
        "throughput": metrics.posts_per_second
    })

    # å¦‚æœé”™è¯¯å¤šï¼Œå¯¼å‡ºæ—¥å¿—
    if metrics.detail_failed > 5:
        metrics.export_error_log(f"errors_{config['workers']}_workers.txt")

# å¯¹æ¯”ç»“æœ
print(f"\n{'é…ç½®':<10} {'æˆåŠŸç‡':<10} {'é”™è¯¯æ•°':<10} {'ååé‡':<15}")
print("-" * 50)
for r in results:
    print(f"{r['name']:<10} {r['success_rate']:<10.2f}% {r['error_count']:<10} {r['throughput']:<15.3f} posts/s")
```

## ğŸ¯ è°ƒè¯•æŠ€å·§

### 1. å¿«é€Ÿå®šä½é—®é¢˜

å½“æµ‹è¯•å¤±è´¥ç‡é«˜æ—¶ï¼ŒæŸ¥çœ‹é”™è¯¯æ‘˜è¦çš„å‰å‡ ä¸ªä¾‹å­ï¼š

```
âŒ ERROR SUMMARY

  ParseError: 15 occurrences
    Example 1: BID: Nzw8xB3k2
              Message: Post Nzw8xB3k2 not found
```

å¦‚æœæ‰€æœ‰é”™è¯¯éƒ½æ˜¯ "Post not found"ï¼Œè¯´æ˜æœç´¢è¿”å›çš„ BID æ˜¯æ— æ•ˆçš„ã€‚

### 2. æ—¶é—´åºåˆ—åˆ†æ

å¯¼å‡ºé”™è¯¯æ—¥å¿—åï¼Œå¯ä»¥åˆ†æé”™è¯¯å‘ç”Ÿçš„æ—¶é—´æ¨¡å¼ï¼š

```python
import time

# åˆ†æé”™è¯¯æ—¶é—´åˆ†å¸ƒ
if metrics.detail_errors:
    error_times = [err.timestamp for err in metrics.detail_errors]
    start_time = metrics.start_time

    print("\né”™è¯¯æ—¶é—´åˆ†å¸ƒï¼š")
    for i, err in enumerate(metrics.detail_errors[:10], 1):
        elapsed = err.timestamp - start_time
        print(f"  é”™è¯¯ {i}: ç¬¬ {elapsed:.1f} ç§’, BID: {err.bid}, ç±»å‹: {err.error_type}")
```

å¦‚æœé”™è¯¯é›†ä¸­åœ¨æµ‹è¯•å¼€å§‹æˆ–ç»“æŸï¼Œå¯èƒ½æ˜¯ç‰¹å®šæ—¶é—´æ®µçš„é—®é¢˜ã€‚

### 3. æŒ‰é”™è¯¯ç±»å‹è¿‡æ»¤

```python
# åªæŸ¥çœ‹ç‰¹å®šç±»å‹çš„é”™è¯¯
parse_errors = [err for err in metrics.detail_errors if err.error_type == "ParseError"]
network_errors = [err for err in metrics.detail_errors if err.error_type == "NetworkError"]

print(f"ParseError æ•°é‡: {len(parse_errors)}")
print(f"NetworkError æ•°é‡: {len(network_errors)}")

# æŸ¥çœ‹ ParseError çš„å…·ä½“æ¶ˆæ¯
if parse_errors:
    print("\nParseError ç¤ºä¾‹ï¼š")
    for err in parse_errors[:5]:
        print(f"  BID: {err.bid}")
        print(f"  æ¶ˆæ¯: {err.error_message}")
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### åŸºäºé”™è¯¯åˆ†æä¼˜åŒ–

1. **å¦‚æœ NetworkError å¾ˆå¤šï¼š**
   - é™ä½å¹¶å‘æ•°
   - æ›´æ¢ä»£ç†æœåŠ¡å•†
   - å¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´

2. **å¦‚æœ ParseError å¾ˆå¤šï¼š**
   - æ£€æŸ¥ API æ˜¯å¦å˜åŒ–
   - éªŒè¯æœç´¢ç»“æœçš„ BID æ˜¯å¦æœ‰æ•ˆ
   - æ£€æŸ¥ cookies æ˜¯å¦è¿‡æœŸ

3. **å¦‚æœç‰¹å®š BID é‡å¤å¤±è´¥ï¼š**
   - å¯èƒ½æ˜¯è¯¥åšæ–‡å·²è¢«åˆ é™¤
   - å¯èƒ½æ˜¯æƒé™é—®é¢˜ï¼ˆç§å¯†åšæ–‡ï¼‰
   - è¿‡æ»¤æ‰è¿™äº› BID

### Worker æ•°é‡ä¼˜åŒ–

ç°åœ¨ client å¤ç”¨åï¼Œå¯ä»¥æ›´å®‰å…¨åœ°å¢åŠ å¹¶å‘æ•°ï¼š

```python
# æµ‹è¯•æœ€ä¼˜ worker æ•°é‡
for workers in [1, 3, 5, 10, 15, 20, 30]:
    metrics = crawler.crawl_topic_concurrent(
        query="æµ‹è¯•",
        search_pages=2,
        max_posts=50,
        num_workers=workers
    )

    print(f"{workers} workers: {metrics.posts_per_second:.3f} posts/s, "
          f"{metrics.detail_success_rate:.1f}% æˆåŠŸç‡")

    # å¦‚æœæˆåŠŸç‡å¼€å§‹ä¸‹é™ï¼Œè¯´æ˜è¾¾åˆ°ç“¶é¢ˆ
    if metrics.detail_success_rate < 85:
        print(f"âš ï¸  {workers} ä¸ª workers æ—¶æˆåŠŸç‡ä¸‹é™åˆ° {metrics.detail_success_rate:.1f}%")
        break
```

## ğŸ“ æœ€ä½³å®è·µ

1. **å§‹ç»ˆæ£€æŸ¥é”™è¯¯æ‘˜è¦**ï¼šæ¯æ¬¡æµ‹è¯•åæŸ¥çœ‹é”™è¯¯ç±»å‹å’Œæ•°é‡
2. **æˆåŠŸç‡ < 90% æ—¶å¯¼å‡ºæ—¥å¿—**ï¼šè¯¦ç»†åˆ†æé—®é¢˜åŸå› 
3. **è®°å½•æµ‹è¯•é…ç½®å’Œç»“æœ**ï¼šä¾¿äºå¯¹æ¯”å’Œä¼˜åŒ–
4. **æ¸è¿›å¼å¢åŠ å‹åŠ›**ï¼šä»å°å¹¶å‘å¼€å§‹ï¼Œè§‚å¯Ÿé”™è¯¯ç‡å˜åŒ–
5. **ä½¿ç”¨ä¸åŒå…³é”®è¯æµ‹è¯•**ï¼šé¿å…å•ä¸€æµ‹è¯•åœºæ™¯çš„åå·®

## ğŸ“Œ æ³¨æ„äº‹é¡¹

- é”™è¯¯æ—¥å¿—å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ BIDï¼‰ï¼Œåˆ†äº«å‰è¯·æ£€æŸ¥
- å¤§é‡é”™è¯¯æ—¶æ—¥å¿—æ–‡ä»¶å¯èƒ½å¾ˆå¤§ï¼Œæ³¨æ„ç£ç›˜ç©ºé—´
- é”™è¯¯æ—¶é—´æˆ³ä½¿ç”¨ Unix timestampï¼Œå¯ç”¨ `datetime.fromtimestamp()` è½¬æ¢
